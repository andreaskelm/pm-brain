# Cognitive Bias Framework for Product Management

## Overview

This framework helps product teams recognize and mitigate cognitive biases that distort decision-making. Cognitive biases are NOT character flawsâ€”they're systematic patterns of thinking that all humans experience.

## Step 0: Braindump & Reflect (Critical!)

**Braindump first:** See [PRODUCT-SENSE-RULES.md](../../../../PRODUCT-SENSE-RULES.md) for the golden rule.

**Use prompts from:** [2-product-sense-prompts.md](../../2.0.1-Mental-Models/6-Product-Sense-Development/2-product-sense-prompts.md) â†’ [Generic Step 0 (any framework)](../../2.0.1-Mental-Models/6-Product-Sense-Development/2-product-sense-prompts.md#generic-step-0-any-framework).

**Quick start:** What decision are you making? What does your product sense tell you? What assumptions are you making? What biases might be affecting you? If you had to defend this to a skeptic, what would you say? What would make you say "obviously biased" or "obviously sound"? **See prompts file for full list.**

## Core Philosophy

### Biases are NOT Evil

Understanding cognitive biases should:

- **Enable awareness over blame** - Recognize patterns, donâ€™t shame people
- **Drive debiasing over perfection** - Reduce impact, not eliminate entirely
- **Foster humility over confidence** - Accept our minds have blind spots
- **Encourage systems over willpower** - Build processes that counteract bias

### Understanding the Bias Landscape

Biases can be categorized by their impact:

- **DECISION BIASES** - Affect how we choose between options
- **JUDGMENT BIASES** - Distort how we assess information
- **MEMORY BIASES** - Influence what we remember and how
- **SOCIAL BIASES** - Shape how we view others and groups

## Framework Structure

### 1. Header Section

Always include:

- **Disclaimer** - â€œEveryone has biases; awareness is the first step to better decisionsâ€
- **Bias category** - Which type of bias is this?
- **Risk level** - How dangerous is this bias in your context?
- **Last reviewed date**

### 2. Critical Biases (Must-Know for PMs)

**Confirmation Bias (Most Dangerous)**

- Seeking information that confirms existing beliefs
- Ignoring contradictory evidence
- High risk: Builds wrong products

**Anchoring Bias (Negotiations & Pricing)**

- Over-relying on first information received
- Sticky initial impressions
- High risk: Poor pricing, bad estimates

**Sunk Cost Fallacy (Project Continuation)**

- Continuing investment due to past costs
- â€œThrowing good money after badâ€
- High risk: Wasting resources on failing initiatives

**Availability Heuristic (Fire-Fighting)**

- Overweighting recent or memorable events
- Recency bias in decision-making
- Medium risk: Reactive prioritization

**Optimism Bias (Estimation)**

- Underestimating risks and timelines
- Overestimating success probability
- High risk: Missed deadlines, disappointed stakeholders

### 3. Social & Group Biases

**Bandwagon Effect (Groupthink)**

- Following othersâ€™ opinions/decisions
- â€œEveryone else is doing itâ€
- Medium risk: Poor differentiation

**Halo Effect (Overvaluing Opinions)**

- Assuming expertise in one area = expertise in all
- Being swayed by credentials/charisma
- Medium risk: Bad advisory input

**Authority Bias (HiPPO)**

- Overvaluing opinions of authority figures
- â€œBecause the CEO said soâ€
- High risk: Ignoring data for opinion

### 4. Time & Memory Biases

**Recency Bias (Latest = Greatest)**

- Recent events seem more important
- Forgetting historical patterns
- Medium risk: Short-term thinking

**Hindsight Bias (â€œI Knew It All Alongâ€)**

- Believing past events were predictable
- Overconfidence in future predictions
- Low risk: False learnings

## Writing Guidelines

### Bias Identification

- âœ… â€œI notice Iâ€™m seeking data that confirms my hypothesisâ€
- âœ… â€œThe team is anchored on the first pricing model discussedâ€
- âœ… â€œWeâ€™re continuing this project because weâ€™ve already invested 6 monthsâ€
- âŒ â€œMy gut says this is rightâ€
- âŒ â€œI just know this will workâ€
- âŒ â€œTrust me on thisâ€

### Debiasing Strategies

- âœ… â€œLetâ€™s actively seek disconfirming evidenceâ€
- âœ… â€œWhat if we started this pricing discussion fresh?â€
- âœ… â€œIgnore sunk costs: would we start this project today with what we know?â€
- âŒ â€œTry harder to be objectiveâ€
- âŒ â€œJust donâ€™t be biasedâ€
- âŒ â€œThink rationallyâ€

### Decision Documentation

- âœ… â€œWe decided X based on [data/evidence]. Potential biases: [list]. Mitigation: [strategy]â€
- âœ… â€œAssumptions: [list]. How weâ€™ll validate: [method]â€
- âŒ â€œWe just felt this was rightâ€
- âŒ â€œEveryone agreedâ€

## Review Schedule

### Pre-Decision Check (Before Major Decisions)

- Which biases might affect this decision?
- What contrary evidence have we sought?
- Have we consulted diverse perspectives?
- Are we anchored on anything?

### Weekly Team Reviews

- Recent decisions made
- Biases that may have influenced them
- Outcomes vs. predictions
- Calibration adjustments

### Monthly Retrospectives

- Major decisions review
- Bias patterns identification
- Team debiasing effectiveness
- Process improvements

### Quarterly Bias Audits

- Full decision portfolio review
- Systematic bias patterns
- Cultural factors enabling biases
- Training needs assessment

## Stakeholder Communication

### For Leadership

- Frame biases as universal, not personal failures
- Show how debiasing improves decision quality
- Highlight ROI of bias awareness training
- Make bias discussion psychologically safe

### For Product Teams

- Normalize bias discussions in every meeting
- Use bias checklists before decisions
- Celebrate bias-catching, not perfection
- Share personal bias experiences

### For Engineering Teams

- Focus on estimation biases (optimism, anchoring)
- Use reference class forecasting
- Track actual vs. estimated effort
- Calibrate predictions over time

### For Design Teams

- Address confirmation bias in user research
- Seek disconfirming evidence actively
- Use diverse research methods
- Question assumptions regularly

## Common Challenges and Solutions

### â€œI donâ€™t have biasesâ€”Iâ€™m objectiveâ€

**Response pattern:**
â€œEveryone has biasesâ€”itâ€™s how human brains work. Even being aware of biases doesnâ€™t eliminate them. The goal isnâ€™t perfection, itâ€™s awareness and systematic debiasing through process.â€

### â€œCalling out biases feels like personal attacksâ€

**Response pattern:**
â€œBiases arenâ€™t character flawsâ€”theyâ€™re cognitive shortcuts our brains use. When we name a bias, weâ€™re identifying a pattern, not judging a person. Letâ€™s focus on improving decisions, not blaming individuals.â€

### â€œWe donâ€™t have time to check for every biasâ€

**Response pattern:**
â€œWe donâ€™t need to check for every bias every time. Focus on the highest-risk biases for each decision type. Use checklists for big decisions, quick checks for small ones.â€

### â€œOur culture doesnâ€™t support admitting mistakesâ€

**Response pattern:**
â€œBias work requires psychological safety. Letâ€™s start by leadership modeling vulnerabilityâ€”sharing their own biases and mistakes. Celebrate bias-catching as a positive behavior.â€

## Best Practices

### Doâ€™s

- Assume everyone (including you) is biased
- Create pre-decision bias checklists
- Seek diverse perspectives actively
- Document assumptions and test them
- Use data to challenge intuitions
- Assign â€œdevilâ€™s advocateâ€ roles
- Track predictions vs. outcomes to calibrate
- Make bias discussion routine and safe

### Donâ€™ts

- Assume awareness eliminates bias
- Use bias accusations as weapons
- Only check for bias after bad outcomes
- Rely on willpower alone to overcome bias
- Skip the debiasing step when rushed
- Punish people for having biases
- Ignore organizational/cultural factors
- Think youâ€™re immune because youâ€™re smart

## Bias Detection Metrics

Track these to know if your debiasing efforts are working:

- **Decision quality** - Are outcomes improving?
- **Prediction accuracy** - Actual vs. estimated outcomes
- **Diverse input** - % of decisions with multi-perspective input
- **Bias awareness** - Team survey: â€œCan you name 3 biases that affected recent decisions?â€
- **Course corrections** - How often do we change course based on new evidence?
- **Psychological safety** - Team comfort admitting biases and mistakes

## Cognitive Bias Codex

### The Four Problems Our Brains Try to Solve

**Problem 1: Too Much Information**
Our brains filter information aggressively to cope

**Biases that result:**

- Confirmation Bias
- Availability Heuristic
- Anchoring Bias
- Attentional Bias
- Framing Effect

**Problem 2: Not Enough Meaning**
We fill gaps with assumptions and patterns

**Biases that result:**

- Halo Effect
- Stereotyping
- Clustering Illusion
- Apophenia (seeing patterns in randomness)

**Problem 3: Need to Act Fast**
We take mental shortcuts to decide quickly

**Biases that result:**

- Sunk Cost Fallacy
- Status Quo Bias
- Loss Aversion
- Bandwagon Effect
- Authority Bias

**Problem 4: What to Remember**
We simplify and distort memories

**Biases that result:**

- Hindsight Bias
- Rosy Retrospection
- Recency Bias
- Choice-Supportive Bias

-----

# Cognitive Bias Reference Guide & Template

## Disclaimer

This framework recognizes that ALL humans experience cognitive biases. The goal is not perfection but awareness, systematic debiasing, and better decision-making over time.

**Last updated:** [Date]
**Next review:** [Date]
**Framework owner:** [Name/Role]

-----

## ğŸ¯ Top 10 Biases for Product Managers

### 1. Confirmation Bias âš ï¸ CRITICAL

**Definition:** Seeking information that confirms existing beliefs while ignoring contradictory evidence.

**Why it matters:** Leads to building features nobody wants because you only listened to validating feedback.

**Product management examples:**

- Only interviewing users who love your product
- Asking leading questions in user research
- Dismissing negative feedback as â€œoutliersâ€
- Cherry-picking metrics that show success

**How to detect:**

- Ask: â€œWhat would prove me wrong?â€
- Check: Are we seeking disconfirming evidence?
- Notice: Do we have confirmation bias keywords? (â€œSee, I told you,â€ â€œAs expectedâ€)

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|---------------------------|----------------------------------------|------|-------------|
|**Pre-mortem** |Before launching, assume it failed. Why?|Low |High |
|**Disconfirmation seeking**|Actively look for evidence youâ€™re wrong |Medium|High |
|**Diverse perspectives** |Include skeptics in research/decisions |Medium|High |
|**Red team exercise** |Assign someone to argue the opposite |Low |Medium |
|**Blind data analysis** |Analyze data before forming hypothesis |High |Very High |

**Decision checklist:**

- [ ] Have we actively sought disconfirming evidence?
- [ ] Did we interview users who DONâ€™T use/like our product?
- [ ] Are we asking open-ended, unbiased questions?
- [ ] Have we consulted people who disagree with our hypothesis?
- [ ] Can we name 3 pieces of evidence that contradict our belief?

-----

### 2. Anchoring Bias âš ï¸ CRITICAL

**Definition:** Over-relying on the first piece of information encountered when making decisions.

**Why it matters:** First numbers stickâ€”bad for pricing, estimates, and negotiations.

**Product management examples:**

- First pricing suggestion becomes the â€œrightâ€ price
- Initial effort estimate anchors all subsequent planning
- First feature idea dominates prioritization discussion
- Original launch date becomes immovable despite scope changes

**How to detect:**

- Ask: â€œWhere did this number come from?â€
- Check: Are we adjusting enough from the anchor?
- Notice: Is everyone referencing the first estimate?

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|----------------------------------|---------------------------------------------|------|-------------|
|**Multiple independent estimates**|Get 3+ estimates before discussing |Medium|High |
|**Bottom-up estimation** |Build from components, not top-down |High |Very High |
|**Reference class forecasting** |Use historical data from similar projects |Medium|High |
|**Delay anchors** |Make estimates before hearing othersâ€™ numbers|Low |Medium |
|**Question the anchor** |Explicitly ask â€œWhy this number?â€ |Low |Medium |

**Decision checklist:**

- [ ] Did we generate our own estimate before hearing othersâ€™?
- [ ] Have we used historical data to calibrate?
- [ ] Did we build bottom-up from components?
- [ ] Can we justify this number independently of the anchor?
- [ ] Have we explicitly questioned the first number shared?

-----

### 3. Sunk Cost Fallacy âš ï¸ CRITICAL

**Definition:** Continuing investment in a failing initiative because of past investment (time, money, effort).

**Why it matters:** Wastes resources on doomed projects. Prevents cutting losses.

**Product management examples:**

- Continuing a feature because â€œweâ€™ve already spent 6 months on itâ€
- Not pivoting because of past technical decisions
- Keeping failing products alive due to historical investment
- Finishing projects even when better opportunities emerge

**How to detect:**

- Ask: â€œWould we start this project today knowing what we know?â€
- Check: Are we defending this based on past investment?
- Notice: Phrases like â€œweâ€™ve come too far to stop nowâ€

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|----------------------------|---------------------------------------------|------|-------------|
|**Zero-based thinking** |â€œWould we start this today?â€ |Low |Very High |
|**Kill criteria** |Define failure metrics before starting |Low |High |
|**External perspective** |â€œWhat would we advise a friend?â€ |Low |High |
|**Regular check-ins** |Monthly: â€œDoes this still make sense?â€ |Low |Medium |
|**Opportunity cost framing**|â€œWhat else could we do with these resources?â€|Low |High |

**Decision checklist:**

- [ ] Would we start this project today with current information?
- [ ] Are we defending this based on past investment?
- [ ] Have we calculated opportunity cost?
- [ ] Do we have clear kill criteria?
- [ ] Whatâ€™s the best use of our next dollar/hour?

-----

### 4. Availability Heuristic âš ï¸ HIGH RISK

**Definition:** Overweighting information thatâ€™s easily recalled (recent, dramatic, emotional).

**Why it matters:** Leads to reactive fire-fighting instead of strategic priorities.

**Product management examples:**

- Prioritizing the last customer complaint you heard
- Overreacting to recent incidents
- Basing decisions on one vivid customer story
- Assuming recent trends will continue indefinitely

**How to detect:**

- Ask: â€œIs this actually common or just memorable?â€
- Check: What does the data say about frequency?
- Notice: Are we reacting to the latest fire?

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|-----------------------------|----------------------------------|------|-------------|
|**Base rate analysis** |Check actual frequency in data |Medium|Very High |
|**Cooling off period** |Wait 24-48 hours before reacting |Low |Medium |
|**Diverse feedback channels**|Multiple sources, not just loudest|Medium|High |
|**Statistical thinking** |â€œHow common is this really?â€ |Low |High |
|**Priority framework** |Use RICE/ICE, not recency |Medium|High |

**Decision checklist:**

- [ ] Have we checked how common this actually is?
- [ ] Are we reacting to recency instead of frequency?
- [ ] Did we wait before making this decision?
- [ ] Have we gathered feedback from multiple sources?
- [ ] Does this align with our strategic priorities?

-----

### 5. Optimism Bias âš ï¸ HIGH RISK

**Definition:** Underestimating risks, timelines, and costs; overestimating success probability.

**Why it matters:** Leads to missed deadlines, budget overruns, and disappointed stakeholders.

**Product management examples:**

- â€œThis will only take 2 weeksâ€ (takes 8)
- Underestimating technical complexity
- Assuming high adoption without validation
- Ignoring potential failure modes

**How to detect:**

- Ask: â€œWhat could go wrong?â€
- Check: How have similar estimates performed?
- Notice: Are estimates consistently optimistic?

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness |
|-------------------------------|-----------------------------------|------|----------------|
|**Reference class forecasting**|Use data from past similar projects|Medium|Very High |
|**Pre-mortem** |Assume failure. What went wrong? |Low |High |
|**Outside view** |How long did similar projects take?|Medium|High |
|**Add buffers** |Multiply estimates by 1.5-2x |Low |Medium |
|**Track accuracy** |Compare actuals vs. estimates |Low |High (over time)|

**Decision checklist:**

- [ ] Have we looked at how long similar projects took?
- [ ] Did we do a pre-mortem (assume failure, work backward)?
- [ ] Have we added appropriate buffers?
- [ ] Whatâ€™s our track record on similar estimates?
- [ ] What could go wrong that we havenâ€™t considered?

-----

### 6. Bandwagon Effect / Groupthink ğŸŸ¡ MEDIUM RISK

**Definition:** Following othersâ€™ opinions/decisions because â€œeveryoneâ€™s doing it.â€

**Why it matters:** Leads to me-too features and poor differentiation.

**Product management examples:**

- Adding features because competitors have them
- Following industry trends without strategic fit
- Team agreement without critical evaluation
- â€œBest practicesâ€ applied blindly

**How to detect:**

- Ask: â€œWhy is this right for US?â€
- Check: Are we just copying others?
- Notice: Phrases like â€œeveryoneâ€™s doing itâ€

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|------------------------|----------------------------------|------|-------------|
|**Strategic fit check** |Does this align with OUR strategy?|Low |High |
|**Devilâ€™s advocate** |Assign someone to argue against |Low |Medium |
|**Silent brainstorming**|Write ideas before discussing |Low |Medium |
|**â€œ5 Whysâ€ technique** |Why do we need this? (5x) |Low |Medium |
|**Dissent channels** |Safe way to disagree |Medium|High |

**Decision checklist:**

- [ ] Does this fit OUR unique strategy?
- [ ] Have we heard dissenting opinions?
- [ ] Did we generate ideas independently first?
- [ ] Can we articulate why this is right for us (not just others)?
- [ ] What would make us unique if we DONâ€™T do this?

-----

### 7. Halo Effect ğŸŸ¡ MEDIUM RISK

**Definition:** Assuming expertise in one area means expertise in all areas.

**Why it matters:** Overvaluing opinions from wrong sources.

**Product management examples:**

- Assuming successful entrepreneur knows your market
- Valuing engineering input on design decisions
- CEO expertise in one domain â†’ trust in all domains
- Listening to charismatic people over quiet experts

**How to detect:**

- Ask: â€œIs this person an expert in THIS specific area?â€
- Check: Whatâ€™s their track record here?
- Notice: Are we deferring due to general reputation?

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|-----------------------------|-------------------------------------|------|-------------|
|**Domain-specific expertise**|Match expert to specific question |Low |High |
|**Blind evaluation** |Evaluate ideas without knowing source|Medium|Very High |
|**Diverse input** |Get multiple expert perspectives |Medium|High |
|**Question credentials** |â€œWhat makes them expert in THIS?â€ |Low |Medium |
|**Evidence-based** |â€œShow me data, not credentialsâ€ |Low |High |

**Decision checklist:**

- [ ] Is this person an expert in this SPECIFIC domain?
- [ ] Have we evaluated ideas blind to source?
- [ ] Did we get diverse expert input?
- [ ] Can they show evidence/data for their position?
- [ ] Are we deferring due to general reputation vs. specific expertise?

-----

### 8. Authority Bias / HiPPO ğŸŸ¡ MEDIUM RISK

**Definition:** Overvaluing opinions of authority figures (Highest Paid Personâ€™s Opinion).

**Why it matters:** Data-driven decisions get overridden by hierarchy.

**Product management examples:**

- â€œBecause the CEO wants itâ€
- Senior stakeholder opinion > user research
- Avoiding disagreement with leadership
- Automatic approval of executive ideas

**How to detect:**

- Ask: â€œWould we do this if [leader] didnâ€™t suggest it?â€
- Check: Are we using data or deferring to hierarchy?
- Notice: Phrases like â€œthe boss wantsâ€

**Debiasing strategies:**

| Strategy | How to Apply | Effort | Effectiveness |
|---|---|---|---|
| **Data-first culture** | Present data before opinions | Medium | High |
| **Anonymous input** | Collect ideas blind to seniority | Medium | High |
| **Disagree and commit** | Norm of respectful disagreement | High | Very High (long-term) |
| **Decision framework** | Use RICE/ICE for all suggestions | Medium | High |
| **Leader modeling** | Execs ask â€œwhat does data say?â€ | Low | High |

**Decision checklist:**

- [ ] Would we prioritize this if it werenâ€™t from [authority]?
- [ ] Have we evaluated using our prioritization framework?
- [ ] Did we present data before opinions?
- [ ] Is there space to respectfully disagree?
- [ ] What does the evidence say (independent of who suggested it)?

-----

### 9. Recency Bias ğŸŸ¢ LOW-MEDIUM RISK

**Definition:** Recent events seem more important/common than they are.

**Why it matters:** Short-term thinking; forgetting important historical patterns.

**Product management examples:**

- Prioritizing yesterdayâ€™s customer complaint
- Forgetting seasonal patterns in data
- Overreacting to latest competitive move
- Ignoring long-term trends for recent blips

**How to detect:**

- Ask: â€œIs this actually new or just recent?â€
- Check: What do longer-term trends show?
- Notice: Are we reacting to the last weekâ€™s data?

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|-----------------------|-------------------------------------|------|-------------|
|**Long-term data view**|Look at 12+ months, not 1 month |Low |High |
|**Trend analysis** |Is this an anomaly or trend? |Medium|High |
|**Cooling off period** |Wait before reacting to â€œurgentâ€ |Low |Medium |
|**Pattern recognition**|â€œHave we seen this before?â€ |Low |Medium |
|**Regular reviews** |Monthly trends vs. daily fluctuations|Low |High |

**Decision checklist:**

- [ ] Have we looked at longer-term data (12+ months)?
- [ ] Is this a trend or an anomaly?
- [ ] Have we seen this pattern before? What happened?
- [ ] Are we reacting too quickly to recent events?
- [ ] What does the strategic roadmap say vs. latest fire?

-----

### 10. Loss Aversion ğŸŸ¢ LOW-MEDIUM RISK

**Definition:** Fear of losses weighs more than equivalent gains.

**Why it matters:** Overly conservative; missing opportunities.

**Product management examples:**

- Not killing failing products (fear of admitting failure)
- Avoiding beneficial risks
- Resistance to change from status quo
- Staying with known solutions vs. trying better ones

**How to detect:**

- Ask: â€œAre we avoiding this because of fear?â€
- Check: Whatâ€™s the actual risk vs. perceived risk?
- Notice: Defensive language about change

**Debiasing strategies:**

|Strategy |How to Apply |Effort|Effectiveness|
|------------------------|---------------------------------------|------|-------------|
|**Frame as gains** |Focus on what we gain, not what we lose|Low |Medium |
|**Calculated risk** |Quantify actual risk vs. reward |Medium|High |
|**Small experiments** |Test without full commitment |Low |High |
|**Portfolio approach** |Some bets should be risky |Medium|High |
|**Prospective thinking**|â€œWhat would new PM/CEO do?â€ |Low |Medium |

**Decision checklist:**

- [ ] Are we being overly risk-averse?
- [ ] Have we quantified the actual risk?
- [ ] Whatâ€™s the opportunity cost of inaction?
- [ ] Can we test this with a small experiment first?
- [ ] What would a new leader do without our history?

-----

## ğŸ“‹ Bias Decision Checklist

### Before Any Major Decision

**Preparation (5-10 minutes):**

- [ ] Which biases are most likely to affect this decision?
- [ ] Who are the stakeholders and what biases might they bring?
- [ ] What process will we use to counteract biases?

**During Discussion (Throughout):**

- [ ] Are we seeking disconfirming evidence (confirmation bias)?
- [ ] Are we anchored on a number or idea (anchoring)?
- [ ] Are we defending past investments (sunk cost)?
- [ ] Are we reacting to something recent/memorable (availability)?
- [ ] Are our estimates optimistic (optimism bias)?
- [ ] Are we following the crowd (bandwagon)?
- [ ] Are we deferring to authority without data (HiPPO)?

**After Decision (Document):**

- [ ] What assumptions are we making?
- [ ] What biases might have influenced us?
- [ ] How will we test our assumptions?
- [ ] What would make us change our minds?

-----

## ğŸ“ Bias Training Program

### Week 1: Awareness

- Read: This framework
- Activity: Identify 3 biases youâ€™ve experienced
- Practice: Label biases in this weekâ€™s decisions

### Week 2-4: Top 5 Deep-Dives

- One bias per week (confirmation, anchoring, sunk cost, availability, optimism)
- Case studies from your domain
- Apply debiasing checklist

### Month 2: Advanced Biases

- Social biases (bandwagon, halo, authority)
- Judgment biases (recency, loss aversion)
- Memory biases (hindsight, rosy retrospection)

### Month 3: Team Practice

- Bias workshops with cross-functional teams
- Real decision retrospectives
- Create team-specific debiasing protocols

### Ongoing: Calibration

- Track predictions vs. outcomes
- Adjust estimation methods
- Share learnings across team

-----

## ğŸ”¬ Bias Audit Template

### Quarterly Bias Review

**Period:** [Q1/Q2/Q3/Q4 20XX]

**Major Decisions Made:** [Count]

**Bias Analysis:**

|Decision |Date |Potential Biases|Debiasing Used?|Outcome |Learning |
|----------|------|----------------|---------------|--------|---------|
|[Decision]|[Date]|[Biases] |Y/N |[Result]|[Insight]|
| | | | | | |

**Pattern Analysis:**

- Most common bias observed: [Bias name]
- Least debiased decisions: [Examples]
- Most effective debiasing strategy: [Strategy]
- Team strengths: [What we do well]
- Team growth areas: [Where to improve]

**Actions for Next Quarter:**

1. [Specific action based on patterns]
1. [Specific action based on patterns]
1. [Specific action based on patterns]

-----

## ğŸ“š Complete Bias Reference

### Decision-Making Biases

|Bias |Definition |PM Example |Risk Level|Debiasing Strategy |
|--------------------------|---------------------------------|-----------------------------|----------|------------------------------|
|**Confirmation Bias** |Seeking confirming evidence |Only interviewing fans |âš ï¸ Critical|Seek disconfirming evidence |
|**Anchoring Bias** |Over-relying on first info |First price sticks |âš ï¸ Critical|Multiple independent estimates|
|**Sunk Cost Fallacy** |Continuing due to past investment|â€œWeâ€™ve spent 6 monthsâ€ |âš ï¸ Critical|â€œWould we start this today?â€ |
|**Availability Heuristic**|Overweighting recent/memorable |Last complaint = top priority|ğŸŸ¡ High |Check base rates |
|**Optimism Bias** |Underestimating risks |â€œOnly takes 2 weeksâ€ |ğŸŸ¡ High |Reference class forecasting |
|**Status Quo Bias** |Preferring current state |Resistance to change |ğŸŸ¡ Medium |Prospective thinking |
|**Loss Aversion** |Fear of losses > equivalent gains|Wonâ€™t kill failing product |ğŸŸ¢ Low-Med |Frame as gains |

### Social & Group Biases

|Bias |Definition |PM Example |Risk Level|Debiasing Strategy |
|---------------------------------|--------------------------------|-------------------------------|----------|-------------------------|
|**Bandwagon Effect** |Following the crowd |â€œCompetitors have itâ€ |ğŸŸ¡ Medium |Strategic fit check |
|**Halo Effect** |One strength = all strengths |Successful CEO = product expert|ğŸŸ¡ Medium |Domain-specific expertise|
|**Authority Bias (HiPPO)** |Deferring to authority |â€œBecause boss wants itâ€ |ğŸŸ¡ Medium |Data-first culture |
|**Groupthink** |Consensus over critical thinking|No dissent in meetings |ğŸŸ¡ Medium |Devilâ€™s advocate |
|**In-Group Bias** |Favoring own group |Our teamâ€™s ideas > others |ğŸŸ¢ Low |Blind evaluation |
|**Fundamental Attribution Error**|Blaming people, not systems |â€œTheyâ€™re incompetentâ€ |ğŸŸ¢ Low |Systems thinking |

### Time & Memory Biases

|Bias |Definition |PM Example |Risk Level|Debiasing Strategy |
|----------------------|---------------------|--------------------------------|----------|---------------------------|
|**Recency Bias** |Recent = important |Yesterdayâ€™s data drives decision|ğŸŸ¢ Low-Med |Long-term data view |
|**Hindsight Bias** |â€œI knew it all alongâ€|Post-launch overconfidence |ğŸŸ¢ Low |Pre-decision documentation |
|**Rosy Retrospection**|Past seems better |â€œOld product was betterâ€ |ğŸŸ¢ Low |Historical data review |
|**Planning Fallacy** |Underestimating time |Optimistic schedules |ğŸŸ¡ High |Reference class forecasting|

### Judgment & Estimation Biases

|Bias |Definition |PM Example |Risk Level|Debiasing Strategy |
|-------------------------|--------------------------------|-------------------------------------|----------|-------------------------|
|**Overconfidence Bias** |Overestimating own abilities |â€œI can estimate perfectlyâ€ |ğŸŸ¡ Medium |Track prediction accuracy|
|**Dunning-Kruger Effect**|Low competence = high confidence|New PM overestimates skills |ğŸŸ¡ Medium |Seek expert feedback |
|**Framing Effect** |How info is presented matters |â€œ90% successâ€ vs â€œ10% failureâ€ |ğŸŸ¡ Medium |Reframe in multiple ways |
|**Attentional Bias** |Limited focus on some info |Only quantitative, ignore qualitative|ğŸŸ¡ Medium |Multiple data sources |

-----

## References

- Strategy: `../../2.1-Strategy/README.md` (Prioritization)
- Discovery: `../../2.2-Discovery/README.md` (Opportunity Assessment)
- Mental Models: `../2.0.1-Mental-Models/README.md`
- Library Index: `../0-index.md`
- Self-Reflection: `../2.0.3-Self-Reflection/README.md`
