# Product Prioritization Framework

## Overview

This framework helps product teams make systematic decisions about what to build next. Prioritization is NOT about pleasing everyone—it’s about maximizing value delivery with limited resources.

## Step 0: Braindump & Product Sense (Do this first!)

**Before using RICE/ICE prioritization, braindump:**

- What initiatives are you considering? List everything - don't score yet.
- What does your product sense tell you? Which initiatives feel urgent vs. important?
- What assumptions are you making about impact, effort, and reach? List them explicitly.
- What biases might affect your prioritization? (HiPPO pressure? Recency bias? Squeaky wheel? Status quo?)
- What would great product sense look like here? What would an experienced PM prioritize?

**Product sense exercise:**

- If you could only ship ONE thing in the next quarter, what would it be? Why?
- What would make you say "this prioritization is obviously wrong"?
- What would make you say "this prioritization is obviously right"?
- Which initiatives feel like they'll move the needle vs. just keep things running?

**Bias check:**

- What biases might be affecting your view? (Activity bias? Vanity metrics? Output vs. outcome?)
- Are you prioritizing based on what's loudest or what's most valuable?
- What would someone with different product sense prioritize?

**Capture your initial thoughts:**

- Write down your gut reactions before scoring
- Note what feels high priority vs. low priority intuitively
- Identify any red flags your intuition is raising
- List initiatives that feel like they should be deprioritized

**Then proceed** to use RICE/ICE scoring to structure, validate, and deepen your initial product sense.

---

## Core Philosophy

### Prioritization is NOT Guesswork

Effective prioritization should:

- **Drive strategic focus over tactical firefighting** - Align work to goals, not just urgency
- **Reduce bias over gut feelings** - Use data and frameworks, not HiPPO (Highest Paid Person’s Opinion)
- **Increase transparency over politics** - Make decisions defensible and repeatable
- **Enable team autonomy over micromanagement** - Teams understand the “why” behind priorities

### Understanding Prioritization Dimensions

Every initiative can be evaluated across multiple dimensions:

- **VALUE ←→ EFFORT** - What you gain vs. what it costs
- **CERTAINTY ←→ RISK** - Confidence in estimates and outcomes
- **STRATEGIC ←→ TACTICAL** - Long-term positioning vs. short-term needs
- **BROAD ←→ NARROW** - How many users/customers affected

## Framework Structure

### 1. Header Section

Always include:

- **Disclaimer** - “Priorities reflect current strategy and will shift as we learn”
- **Prioritization method** - Which framework(s) are you using?
- **Last prioritization date**
- **Next review date**
- **Decision owner**

### 2. Prioritization Methods

**RICE Scoring (Comprehensive)**

- Best for: Feature development, product roadmaps
- Factors: Reach, Impact, Confidence, Effort
- Time investment: Medium to high
- Bias reduction: High

**ICE Scoring (Quick & Simple)**

- Best for: Growth experiments, rapid testing
- Factors: Impact, Confidence, Ease
- Time investment: Low
- Bias reduction: Medium

**Effort-Impact Matrix (Visual)**

- Best for: Workshops, stakeholder alignment
- Factors: Impact (value) vs. Effort (cost)
- Time investment: Low
- Bias reduction: Low to medium

**Value vs. Complexity (Strategic)**

- Best for: Long-term planning, portfolio decisions
- Factors: Business value vs. Technical complexity
- Time investment: Medium
- Bias reduction: Medium

### 3. Responsibility Assignment (RACI)

**Use when:**

- Multiple stakeholders involved
- Unclear ownership causing delays
- Cross-functional dependencies
- Need decision clarity

**Structure:**

- R = Responsible (does the work)
- A = Accountable (owns the outcome)
- C = Consulted (provides input)
- I = Informed (kept in the loop)

### 4. Decision Criteria

Define what matters most:

- Strategic alignment (does this support our strategy?)
- Customer value (does this solve a real problem?)
- Business impact (does this move key metrics?)
- Technical feasibility (can we actually build this?)
- Resource availability (do we have capacity?)

## How to Use This Framework

### Step 1: Prepare Your Backlog

**Purpose:** Gather all initiatives that need prioritization

**Activities:**
- Collect all candidate initiatives from stakeholders, backlog, and ideas
- Ensure each initiative has a clear description and goal
- Remove duplicates and consolidate similar items
- Get initial effort estimates from engineering (rough is fine)

**Deliverables:**
- Complete list of initiatives to prioritize
- Basic descriptions and goals for each

**Time estimate:** 1-2 hours

### Step 2: Choose Your Scoring Method

**Purpose:** Select the right prioritization framework for your context

**Activities:**
- **Use RICE** when: You have clear reach data, need comprehensive scoring, have time for detailed analysis
- **Use ICE** when: Quick prioritization needed, early-stage ideas, growth experiments, limited time
- **Use Effort-Impact Matrix** when: Visual alignment needed, workshop setting, rough prioritization

**Deliverables:**
- Decision on which method(s) to use
- Scoring criteria defined

**Time estimate:** 15-30 minutes

### Step 3: Score Initiatives

**Purpose:** Calculate objective scores for comparison

**Activities:**
- Score each initiative using chosen method (RICE or ICE)
- For RICE: Estimate Reach, Impact, Confidence, Effort
- For ICE: Rate Impact, Confidence, Ease
- Score collaboratively with cross-functional team
- Document assumptions behind each score

**Deliverables:**
- Scored list of initiatives
- Assumptions documented

**Time estimate:** 2-4 hours for 20-30 initiatives

### Step 4: Review and Validate Scores

**Purpose:** Ensure scores reflect reality and team alignment

**Activities:**
- Review scores for outliers (very high or very low)
- Challenge assumptions with data when possible
- Discuss discrepancies as a team
- Adjust scores based on team input and evidence

**Deliverables:**
- Validated scores
- Team alignment on priorities

**Time estimate:** 1-2 hours

### Step 5: Prioritize and Sequence

**Purpose:** Create ordered priority list and assign to timeframes

**Activities:**
- Sort initiatives by score (highest first)
- Group into NOW/NEXT/LATER based on capacity
- Consider dependencies and sequencing needs
- Balance portfolio (quick wins vs. big bets)

**Deliverables:**
- Prioritized list
- Timeline assignment (NOW/NEXT/LATER)

**Time estimate:** 1 hour

### Step 6: Communicate and Document

**Purpose:** Share priorities and rationale with stakeholders

**Activities:**
- Document final priorities with scores and rationale
- Communicate to stakeholders (leadership, teams, customers)
- Explain what's NOT being prioritized and why
- Set up review cadence

**Deliverables:**
- Prioritization document
- Stakeholder communication complete

**Time estimate:** 1-2 hours

## When to Use RICE/ICE Prioritization

### RICE/ICE Works Best For:

- **Feature prioritization** - When deciding which features to build next
  - **Example:** Choosing between 20 potential features for next quarter
- **Backlog management** - Organizing and ranking existing backlog items
  - **Example:** Quarterly backlog review and prioritization
- **Resource allocation** - Deciding where to invest engineering capacity
  - **Example:** Allocating team capacity across multiple initiatives
- **Stakeholder alignment** - Creating objective basis for priority discussions
  - **Example:** Resolving conflicts between different stakeholder requests
- **Portfolio balance** - Ensuring mix of quick wins and strategic bets
  - **Example:** Quarterly planning to balance risk and reward

### RICE/ICE May Not Be Ideal For:

- **Very small backlogs (<5 items)** - Overhead not worth it
  - **Alternative:** Simple discussion and consensus
- **Highly strategic decisions** - When factors beyond reach/impact matter most
  - **Alternative:** Strategic frameworks (Strategy Blocks, Playing to Win)
- **Emergency situations** - When speed matters more than rigor
  - **Alternative:** Quick triage and immediate action
- **Exploratory research** - When you don't know what you're building yet
  - **Alternative:** Discovery frameworks, opportunity assessment
- **Single-criterion decisions** - When only one factor matters (e.g., compliance)
  - **Alternative:** Simple yes/no decision framework

### Decision Criteria

Use RICE/ICE when:
- ✅ You have multiple initiatives to compare
- ✅ You need objective, defensible prioritization
- ✅ You have some data or estimates available
- ✅ Stakeholder alignment is important
- ✅ You have time for structured process

Consider alternatives when:
- ❌ Backlog is very small (<5 items)
- ❌ Decision is purely strategic (not tactical)
- ❌ Emergency requires immediate action
- ❌ Still in discovery/exploration phase
- ❌ Only one criterion matters

## Writing Guidelines

### Initiative Names

- ✅ “Add multi-factor authentication for enterprise users”
- ✅ “Redesign checkout flow to reduce abandonment”
- ✅ “Build API integration with Salesforce”
- ❌ “Security improvements”
- ❌ “Better UX”
- ❌ “Technical debt”

### Scoring Rationale

- ✅ “Reach: 5,000 enterprise users × 90% adoption = 4,500/quarter”
- ✅ “Impact: 3x (massive) - Directly addresses #1 churn reason”
- ✅ “Effort: 4 person-months based on engineering estimate”
- ❌ “This seems important”
- ❌ “High impact”
- ❌ “Won’t take long”

### Priority Decisions

- ✅ “Prioritized due to RICE score of 625 and strategic alignment with enterprise expansion”
- ✅ “Deprioritized despite stakeholder interest due to low reach (50 users) and high effort (8 person-months)”
- ❌ “Leadership wants this”
- ❌ “Competitors have it”

## Review Schedule

### Weekly Check-ins (Active Work)

- Progress on prioritized initiatives
- Blockers and impediments
- Quick wins or deprioritization needs

### Bi-weekly Backlog Grooming

- Score new ideas
- Re-evaluate scores based on learnings
- Remove obsolete items

### Monthly Strategic Review

- Portfolio balance check
- Resource allocation
- Stakeholder alignment

### Quarterly Planning

- Full re-prioritization
- Strategy alignment check
- Capacity planning
- Major initiative sequencing

## Stakeholder Communication

### For Leadership

- Focus on strategic initiatives and business impact
- Show portfolio balance (quick wins vs. big bets)
- Highlight trade-offs and opportunity costs
- Use scores to defend decisions

### For Product Teams

- Emphasize customer value and impact
- Connect work to metrics and outcomes
- Show how initiatives ladder up to strategy
- Make scoring transparent and participatory

### For Engineering Teams

- Provide technical context and effort estimates
- Show why high-effort items are worth it
- Balance innovation with maintenance
- Make dependencies visible

### For Customer-Facing Teams

- Connect priorities to customer pain points
- Share timeline expectations
- Explain what’s NOT being built and why
- Gather input for future prioritization

## Common Pitfalls and Solutions

### “Everything is a priority”

**Response pattern:**
“If everything is a priority, nothing is. Let’s use RICE scoring to objectively rank these. Which initiatives have the highest reach × impact relative to effort? What are we willing to say ‘not now’ to?”

### “But [executive/customer] really wants this”

**Response pattern:**
“Let’s score it objectively. What’s the reach, impact, confidence, and effort? How does it compare to our current priorities? If it scores higher, we can re-prioritize. If not, we can explain why with data.”

### “We need to balance quick wins with strategic bets”

**Response pattern:**
“Agreed. Let’s aim for 70-20-10: 70% high-confidence improvements, 20% medium-confidence bets, 10% experimental long-shots. Does our current portfolio match this?”

### “The scoring feels subjective and gameable”

**Response pattern:**
“You’re right to be concerned. Let’s: (1) Use consistent scoring scales, (2) Score collaboratively as a team, (3) Base estimates on data when possible, (4) Review scores against actual results to calibrate.”

## Best Practices

### Do’s

- Score collaboratively with cross-functional teams
- Use data to support estimates when available
- Re-score based on learnings and new information
- Make scoring scales consistent across initiatives
- Document assumptions behind scores
- Review actual outcomes vs. predicted scores
- Balance portfolio across effort/impact quadrants

### Don’ts

- Let HiPPO (Highest Paid Person’s Opinion) override scores
- Score in isolation without team input
- Set scores once and never revisit
- Game the system to get pet projects approved
- Ignore effort estimates from engineering
- Forget to communicate deprioritization decisions
- Prioritize solely based on ease (avoid “easy button” trap)

## Prioritization Success Metrics

Track these to know if your framework is working:

- **Delivery predictability** - Are we completing prioritized work on time?
- **Value realization** - Do completed initiatives move target metrics?
- **Team alignment** - Can everyone explain current priorities?
- **Decision speed** - How fast can we make priority calls?
- **Stakeholder satisfaction** - Do stakeholders understand trade-offs?
- **Score accuracy** - How close are actual results to predictions?

## RICE Scoring Deep Dive

### Reach: How many people will this impact?

**Definition:** Number of people/events affected per time period (usually per quarter)

**How to estimate:**

- Use product analytics for existing flows
- Base on current user segments
- Account for adoption rate (not everyone will use it)
- Be specific: “customers/quarter” or “sessions/month”

**Scoring guidance:**

- Use actual numbers, not T-shirt sizes
- 1,000+ users per quarter = significant reach
- 100-1,000 = moderate reach
- <100 = low reach (may still be valuable for strategic reasons)

**Example:**

- Feature used by 2,000 customers/month = 6,000 reach per quarter
- New signup flow with 500 signups/month × 80% see change = 1,200 reach per quarter

### Impact: How much will this improve the metric we care about?

**Definition:** How much each person/event is affected

**Scoring scale:**

- **3.0** = Massive impact - Game-changing, primary differentiator
- **2.0** = High impact - Significant improvement to key metric
- **1.0** = Medium impact - Noticeable improvement
- **0.5** = Low impact - Minor improvement
- **0.25** = Minimal impact - Barely noticeable

**How to estimate:**

- Connect to North Star or input metrics
- Look at similar features’ historical impact
- Use A/B test results when available
- Consider competitive differentiation

**Example:**

- Solves #1 churn reason = 3.0 (massive)
- Improves conversion by 15-20% = 2.0 (high)
- Nice-to-have quality improvement = 0.5 (low)

### Confidence: How sure are you about reach, impact, and effort?

**Definition:** Certainty in your estimates

**Scoring scale:**

- **100%** = High confidence - Strong data, proven approach
- **80%** = Medium confidence - Some data, reasonable assumptions
- **50%** = Low confidence - Mostly assumptions, high uncertainty

**What increases confidence:**

- User research validating the problem
- Historical data on similar features
- Customer requests and feedback
- Technical prototype or proof of concept
- Engineering estimates with specifications

**What decreases confidence:**

- Whiteboard sketches only
- Gut feelings without data
- Untested assumptions
- New technical territory
- External dependencies

**Example:**

- 100%: We have A/B test data showing 25% improvement
- 80%: We have user research but no hard metrics yet
- 50%: This is a hypothesis we want to test

### Effort: How much work will this take?

**Definition:** Total person-months across all functions (product, design, engineering)

**How to estimate:**

- Break down by discipline (PM, design, engineering, QA)
- Use person-months (work one person can do in a month)
- Keep estimates rough: whole numbers or 0.5
- Include all phases: design, development, testing, launch

**Scoring guidance:**

- 0.5 = Less than 2 weeks total effort
- 1 = About 1 month
- 2-3 = Moderate project (quarter)
- 5+ = Major initiative
- 10+ = Multi-quarter epic

**Example:**

- Small UI change: 1 designer (0.5) + 1 engineer (1 week) = 0.5 person-months
- New feature: 2 engineers (6 weeks) + 1 designer (3 weeks) = 3.5 person-months
- Platform rebuild: 4 engineers (3 months) = 12 person-months

### RICE Formula

**RICE Score = (Reach × Impact × Confidence) / Effort**

**Example Calculation:**

Initiative: Add dark mode

- Reach: 80,000 users per quarter
- Impact: 1.0 (medium - quality of life improvement)
- Confidence: 80% (0.8) - we have survey data
- Effort: 2 person-months

RICE Score = (80,000 × 1.0 × 0.8) / 2 = **32,000**

Initiative: AI-powered recommendations

- Reach: 150,000 users per quarter
- Impact: 2.0 (high - drives engagement)
- Confidence: 50% (0.5) - untested hypothesis
- Effort: 6 person-months

RICE Score = (150,000 × 2.0 × 0.5) / 6 = **25,000**

→ Prioritize dark mode (32,000) over AI recommendations (25,000)

## ICE Scoring Alternative

### When to Use ICE Instead of RICE

ICE is faster and simpler, best for:

- Growth experiments and A/B tests
- Early-stage ideas without clear reach data
- Rapid prioritization (minutes, not hours)
- Marketing campaigns and tactics

### ICE Components

**Impact (1-10 scale):**

- 10 = Transformative
- 7-9 = Very high
- 4-6 = Medium
- 1-3 = Low

**Confidence (1-10 scale):**

- 10 = Proven (data-backed)
- 7-9 = High confidence
- 4-6 = Medium confidence
- 1-3 = Low confidence

**Ease (1-10 scale):**

- 10 = Trivial (hours to days)
- 7-9 = Easy (days to week)
- 4-6 = Moderate (1-2 weeks)
- 1-3 = Hard (weeks to months)

**ICE Score = (Impact + Confidence + Ease) / 3**

Or simpler: **Impact × Confidence × Ease**

## RACI Matrix Deep Dive

### When to Use RACI

RACI is essential when:

- Multiple teams/stakeholders involved
- Unclear who makes final decisions
- Frequent “who owns this?” questions
- Cross-functional dependencies
- New initiatives without established ownership

### RACI Roles Explained

**R = Responsible (Doer)**

- Does the actual work
- Completes the task or deliverable
- Can have multiple people
- Cannot delegate their responsibility

**Examples:**

- Engineer writes the code
- Designer creates the mockups
- PM writes the specification

**A = Accountable (Owner)**

- Owns the outcome
- Makes final decisions
- Approves the work
- **ONLY ONE per task**
- Cannot be delegated

**Examples:**

- Product Manager accountable for feature delivery
- Engineering Lead accountable for technical quality
- VP Product accountable for roadmap

**C = Consulted (Advisor)**

- Provides input and expertise
- Two-way communication
- Subject matter experts
- Input happens BEFORE work is done

**Examples:**

- Legal consulted on privacy features
- Security consulted on authentication changes
- Customer success consulted on UX changes

**I = Informed (Stakeholder)**

- Kept updated on progress
- One-way communication
- No input required
- Notified of completion or major milestones

**Examples:**

- Executive team informed of launch
- Marketing informed of feature release
- Support team informed of changes

### RACI Best Practices

**One Accountable Rule:**

- Each task must have exactly ONE accountable person
- This is your decision-maker and ultimate owner
- Two accountables = no accountability

**Not Everyone Needs a Letter:**

- It’s okay to have blank cells
- Not every role needs involvement in every task
- Too many C’s = decision paralysis
- Too many I’s = email overload

**Balance Responsible Assignments:**

- Don’t overload individuals with too many R’s
- Distribute work across the team
- Check for capacity and bandwidth

**RACI is Living Document:**

- Update as project evolves
- Review when responsibilities shift
- Remove completed tasks regularly

## Effort-Impact Matrix

### Four Quadrants

**Quick Wins (High Impact, Low Effort)**

- **Do First**
- Easy wins that move the needle
- Build momentum and team confidence
- Target: 40% of capacity

**Major Projects (High Impact, High Effort)**

- **Do Next**
- Strategic bets worth the investment
- Require cross-functional commitment
- Target: 40% of capacity

**Fill-Ins (Low Impact, Low Effort)**

- **Do Later**
- Nice-to-haves when capacity allows
- Good for new team members
- Target: 10% of capacity

**Money Pits (Low Impact, High Effort)**

- **Avoid**
- High cost, low return
- Question why these exist
- Target: 0% of capacity (unless strategic requirement)

### Using the Matrix

1. Plot all initiatives on the matrix
1. Identify where most items cluster
1. Aim for portfolio balance:
- 40% Quick Wins
- 40% Major Projects
- 10% Fill-Ins
- 10% Innovation/Exploration
1. Ruthlessly cut or defer Money Pits

-----

## Integration with Other Frameworks

### Combining RICE/ICE with OKRs

**When to combine:** When prioritizing initiatives that support OKR Key Results

**How they work together:**
- RICE/ICE provides objective scoring for initiatives
- OKRs provide strategic context and outcome focus
- Initiatives should map to Key Results they support
- Use RICE scores to prioritize within each KR

**Example workflow:**
```
Step 1: Set OKRs with Key Results (e.g., "Increase user engagement by 20%")
Step 2: List initiatives that could support each KR
Step 3: Score initiatives using RICE
Step 4: Prioritize initiatives by RICE score within each KR
Step 5: Build roadmap from top-scoring initiatives
```

### Combining RICE/ICE with Roadmaps

**When to combine:** When building NOW/NEXT/LATER roadmap sections

**How they work together:**
- RICE/ICE scores determine priority order
- Roadmap assigns initiatives to time horizons based on scores
- High RICE scores → NOW section
- Medium scores → NEXT section
- Lower scores → LATER section

**Example workflow:**
```
Step 1: Score all initiatives using RICE
Step 2: Sort by score (highest first)
Step 3: Assign top 5-7 to NOW (high confidence, high scores)
Step 4: Assign next 5-7 to NEXT (medium confidence, good scores)
Step 5: Assign rest to LATER (lower scores, exploratory)
```

### Combining RICE/ICE with Opportunity Assessment

**When to combine:** When evaluating opportunities before detailed prioritization

**How they work together:**
- Opportunity Assessment identifies and evaluates opportunities
- RICE/ICE prioritizes specific initiatives within opportunities
- Use Opportunity Assessment for strategic fit
- Use RICE/ICE for tactical prioritization

**Example workflow:**
```
Step 1: Use Opportunity Assessment to identify high-value opportunities
Step 2: Generate initiatives that address each opportunity
Step 3: Score initiatives using RICE
Step 4: Prioritize initiatives, ensuring opportunities are addressed
```

### Combining RICE/ICE with MoSCoW Prioritization

**When to combine:** When you need both objective scoring and categorical prioritization

**How they work together:**
- RICE/ICE provides objective scores
- MoSCoW provides categorical buckets (Must/Should/Could/Won't)
- Use RICE scores to inform MoSCoW categorization
- High RICE scores typically = Must Have or Should Have

**Example workflow:**
```
Step 1: Score initiatives using RICE
Step 2: Use RICE scores to inform MoSCoW categories:
   - Top 60% by score → Must Have
   - Next 20% → Should Have
   - Next 20% → Could Have
   - Rest → Won't Have
Step 3: Validate categories make sense strategically
```

### Combining RICE/ICE with Kano Model

**When to combine:** When prioritizing features with different satisfaction impacts

**How they work together:**
- Kano Model categorizes features (Must-Have, Performance, Delighter)
- RICE/ICE scores features within each category
- Prioritize Must-Haves first, then use RICE within categories
- Balance investment across feature types

**Example workflow:**
```
Step 1: Use Kano Model to categorize features
Step 2: Score features using RICE within each category
Step 3: Prioritize: Must-Haves (highest RICE) → Performance (highest RICE) → Delighters (highest RICE)
Step 4: Ensure portfolio includes all three types
```

### Typical Integration Patterns

**Pattern 1: Strategic → Tactical Prioritization**
- Strategy Blocks → OKRs → RICE/ICE → Roadmap
- **Use when:** Building quarterly roadmap from strategy

**Pattern 2: Discovery → Prioritization**
- Opportunity Assessment → RICE/ICE → Roadmap
- **Use when:** Prioritizing discovered opportunities

**Pattern 3: Continuous Prioritization**
- Weekly backlog review → RICE scoring → Update priorities
- **Use when:** Managing ongoing backlog

## References

- RICE/ICE Template: `2-RICE-ICE-scoring-template.md`
- Strategic Foundations: `../2.1.1-Strategic-Foundations/README.md`
- OKR Framework: `../1-OKR/README.md`
- Roadmap: `../2-Roadmap/README.md`
- North Star: `../3-North-Star/README.md`
- MoSCoW Prioritization: `../3-MoSCoW/1-moscow-prioritization-framework.md`
- Kano Model: `../4-Kano-Model/1-kano-framework.md`
- Opportunity Assessment: `../../2.3-Discovery/2.3.4-Opportunity-Assessment/README.md`
- Bias Framework: `../../2.9-Other/2.9.4-Bias/README.md`
- Self-Reflection: `../../2.9-Other/2.9.2-Self-Reflection/README.md`

